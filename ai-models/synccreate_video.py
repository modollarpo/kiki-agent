"""
SyncCreateâ„¢ Video Generation Module
Multi-modal agent for Short-Form Video (TikTok, Reels, YouTube Shorts)

Implements:
- Dynamic Storyboarding (Timeline JSON)
- Motion Synthesis (I2V, Generative B-Roll, Assembly)
- Video Safety Guardrails (Flicker, Audio, Temporal Integrity)
- FFmpeg Integration for overlay & assembly
"""

import os
import json
import subprocess
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime
import requests
import base64
import numpy as np
from PIL import Image
import cv2
from moviepy.editor import ImageClip, CompositeVideoClip, TextClip

# ============================================================================
# VIDEO GENERATION CONFIGURATION
# ============================================================================

@dataclass
class VideoConfig:
    """Configuration for video generation."""
    
    # Format specifications
    aspect_ratio: str = "9:16"  # Vertical for TikTok/Reels
    duration_seconds: int = 15
    fps: int = 30
    resolution: Tuple[int, int] = (1080, 1920)  # Width x Height for 9:16
    
    # Video generation APIs
    video_api: str = "runway"  # runway, luma, svd, mock
    runway_api_key: Optional[str] = None
    luma_api_key: Optional[str] = None
    
    # Asset paths
    output_dir: str = "output/videos"
    temp_dir: str = "output/temp"
    assets_dir: str = "assets"
    
    # Audio settings
    enable_audio: bool = True
    audio_library_path: str = "assets/audio"
    tts_enabled: bool = True
    
    # Safety thresholds
    flicker_threshold: float = 0.15  # Max brightness change between frames
    logo_consistency_threshold: float = 0.85  # Min similarity for logo
    audio_compliance_check: bool = True
    logo_match_threshold: float = 0.7  # Min template match score
    aspect_ratio_tolerance: float = 0.02  # Allowed deviation
    max_duration_seconds: int = 60  # Hard guardrail
    max_file_mb: float = 50.0  # Upload safety limit


@dataclass
class VideoTimeline:
    """Timeline structure for video storyboard."""
    
    hook_start: float = 0.0
    hook_end: float = 2.0
    hook_prompt: str = ""
    
    value_start: float = 2.0
    value_end: float = 7.0
    value_prompt: str = ""
    
    cta_start: float = 7.0
    cta_end: float = 10.0
    cta_prompt: str = ""
    cta_text: str = ""
    
    def to_dict(self) -> Dict:
        """Convert timeline to dictionary."""
        return {
            "segments": [
                {
                    "type": "hook",
                    "start": self.hook_start,
                    "end": self.hook_end,
                    "prompt": self.hook_prompt
                },
                {
                    "type": "value",
                    "start": self.value_start,
                    "end": self.value_end,
                    "prompt": self.value_prompt
                },
                {
                    "type": "cta",
                    "start": self.cta_start,
                    "end": self.cta_end,
                    "prompt": self.cta_prompt,
                    "text_overlay": self.cta_text
                }
            ]
        }


@dataclass
class VideoAssets:
    """Generated video assets."""
    
    video_url: str
    thumbnail_url: str
    audio_track: str
    duration: float
    format: str
    resolution: Tuple[int, int]
    file_size_mb: float
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for payload."""
        return {
            "video_url": self.video_url,
            "thumbnail_url": self.thumbnail_url,
            "audio_track": self.audio_track,
            "duration": f"{self.duration}s",
            "format": self.format,
            "resolution": f"{self.resolution[0]}x{self.resolution[1]}",
            "file_size_mb": round(self.file_size_mb, 2)
        }


# ============================================================================
# DYNAMIC STORYBOARDING
# ============================================================================

class DynamicStoryboard:
    """Generates timeline JSON for Script-to-Scene workflow."""
    
    def __init__(self, config: VideoConfig):
        self.config = config
    
    def generate_timeline(
        self,
        campaign_brief: str,
        product_usp: str,
        target_persona: str,
        cta_text: str = "Shop Now"
    ) -> VideoTimeline:
        """
        Generate strategic timeline aligned with LTV predictions.
        
        Args:
            campaign_brief: Campaign description
            product_usp: Unique selling proposition from AI brain
            target_persona: Target audience persona
            cta_text: Call-to-action text
            
        Returns:
            VideoTimeline with prompts for each segment
        """
        # Generate hook (0-2s): High-motion visual
        hook_prompt = self._generate_hook_prompt(product_usp, target_persona)
        
        # Generate value segment (2-7s): Product USP visualization
        value_prompt = self._generate_value_prompt(product_usp, campaign_brief)
        
        # Generate CTA (7-10s): Brand-safe closing
        cta_prompt = self._generate_cta_prompt(cta_text, target_persona)
        
        timeline = VideoTimeline(
            hook_prompt=hook_prompt,
            value_prompt=value_prompt,
            cta_prompt=cta_prompt,
            cta_text=cta_text
        )
        
        return timeline
    
    def _generate_hook_prompt(self, product_usp: str, persona: str) -> str:
        """Generate high-motion hook prompt to stop the scroll."""
        motion_types = [
            "dynamic camera zoom",
            "swirling motion",
            "product reveal with dramatic lighting",
            "fast-paced product showcase"
        ]
        
        persona_contexts = {
            "young_professional": "modern office setting with natural lighting",
            "fitness_enthusiast": "energetic gym environment with motion blur",
            "creative_professional": "artistic studio with dynamic compositions",
            "tech_early_adopter": "futuristic tech environment with neon accents",
            "default": "engaging lifestyle setting with movement"
        }
        
        context = persona_contexts.get(persona, persona_contexts["default"])
        motion = np.random.choice(motion_types)
        
        return f"{product_usp}, {motion}, {context}, professional cinematography, 4K quality"
    
    def _generate_value_prompt(self, product_usp: str, campaign_brief: str) -> str:
        """Generate value segment showing product in use."""
        return f"{product_usp}, product demonstration, {campaign_brief}, clear product visibility, lifestyle integration, professional lighting"
    
    def _generate_cta_prompt(self, cta_text: str, persona: str) -> str:
        """Generate CTA segment with clear next step."""
        return f"clean minimal background, brand logo prominent, text overlay space for '{cta_text}', professional finish, engaging conclusion"


# ============================================================================
# MOTION SYNTHESIS
# ============================================================================

class MotionSynthesizer:
    """Handles video generation via multiple methods."""
    
    def __init__(self, config: VideoConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.temp_dir = Path(config.temp_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
    
    def image_to_video(
        self,
        image_path: str,
        prompt: str,
        duration: float = 3.0,
        motion_strength: float = 0.7
    ) -> str:
        """
        Convert static image to video with motion (I2V).
        
        Args:
            image_path: Path to input image
            prompt: Motion prompt
            duration: Video duration in seconds
            motion_strength: Intensity of motion (0.0-1.0)
            
        Returns:
            Path to generated video
        """
        if self.config.video_api == "runway":
            return self._runway_i2v(image_path, prompt, duration, motion_strength)
        elif self.config.video_api == "luma":
            return self._luma_i2v(image_path, prompt, duration)
        elif self.config.video_api == "svd":
            return self._svd_i2v(image_path, duration, motion_strength)
        else:
            # Mock mode for testing
            return self._mock_i2v(image_path, duration)
    
    def text_to_video(
        self,
        prompt: str,
        duration: float = 5.0
    ) -> str:
        """
        Generate B-Roll from text prompt.
        
        Args:
            prompt: Video generation prompt
            duration: Video duration in seconds
            
        Returns:
            Path to generated video
        """
        if self.config.video_api == "runway":
            return self._runway_t2v(prompt, duration)
        elif self.config.video_api == "luma":
            return self._luma_t2v(prompt, duration)
        else:
            # Mock mode
            return self._mock_t2v(prompt, duration)
    
    def _runway_i2v(self, image_path: str, prompt: str, duration: float, motion_strength: float) -> str:
        """Generate video using Runway Gen-3 API."""
        if not self.config.runway_api_key:
            print("âš ï¸ Runway API key not set, using mock mode")
            return self._mock_i2v(image_path, duration)
        
        # Runway Gen-3 API endpoint
        url = "https://api.runwayml.com/v1/generations"
        
        # Encode image
        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode()
        
        payload = {
            "model": "gen3",
            "prompt": prompt,
            "image": image_data,
            "duration": duration,
            "motion_strength": motion_strength,
            "aspect_ratio": self.config.aspect_ratio
        }
        
        headers = {
            "Authorization": f"Bearer {self.config.runway_api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            video_url = result.get("output_url")
            
            # Download video
            output_path = self.temp_dir / f"runway_{datetime.now().timestamp()}.mp4"
            self._download_video(video_url, str(output_path))
            
            return str(output_path)
            
        except Exception as e:
            print(f"âš ï¸ Runway API error: {e}, falling back to mock")
            return self._mock_i2v(image_path, duration)
    
    def _luma_i2v(self, image_path: str, prompt: str, duration: float) -> str:
        """Generate video using Luma AI API."""
        if not self.config.luma_api_key:
            print("âš ï¸ Luma API key not set, using mock mode")
            return self._mock_i2v(image_path, duration)
        
        # Luma Dream Machine API
        url = "https://api.lumalabs.ai/v1/generations"
        
        # Implementation similar to Runway
        # For now, fallback to mock
        return self._mock_i2v(image_path, duration)
    
    def _svd_i2v(self, image_path: str, duration: float, motion_strength: float) -> str:
        """Generate video using Stable Video Diffusion (local)."""
        # This would require local SVD model
        # For now, use mock
        return self._mock_i2v(image_path, duration)
    
    def _mock_i2v(self, image_path: str, duration: float) -> str:
        """Mock I2V by creating a static video from image."""
        output_path = self.temp_dir / f"mock_i2v_{datetime.now().timestamp()}.mp4"
        
        # Load image
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Resize to target resolution
        img = cv2.resize(img, self.config.resolution)
        
        # Create video writer
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(
            str(output_path),
            fourcc,
            self.config.fps,
            self.config.resolution
        )
        
        # Write frames (simple zoom effect)
        num_frames = int(duration * self.config.fps)
        for i in range(num_frames):
            # Simple zoom animation
            scale = 1.0 + (i / num_frames) * 0.1  # 10% zoom over duration
            h, w = img.shape[:2]
            center = (w // 2, h // 2)
            
            M = cv2.getRotationMatrix2D(center, 0, scale)
            zoomed = cv2.warpAffine(img, M, (w, h))
            
            out.write(zoomed)
        
        out.release()
        return str(output_path)
    
    def _runway_t2v(self, prompt: str, duration: float) -> str:
        """Text-to-video using Runway."""
        # Similar to I2V but without image input
        return self._mock_t2v(prompt, duration)
    
    def _luma_t2v(self, prompt: str, duration: float) -> str:
        """Text-to-video using Luma."""
        return self._mock_t2v(prompt, duration)
    
    def _mock_t2v(self, prompt: str, duration: float) -> str:
        """Mock T2V by creating a gradient video."""
        output_path = self.temp_dir / f"mock_t2v_{datetime.now().timestamp()}.mp4"
        
        # Create gradient background
        img = np.zeros((self.config.resolution[1], self.config.resolution[0], 3), dtype=np.uint8)
        
        # Add gradient
        for y in range(self.config.resolution[1]):
            color = int(255 * y / self.config.resolution[1])
            img[y, :] = [color // 2, color // 3, color]
        
        # Create video
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(
            str(output_path),
            fourcc,
            self.config.fps,
            self.config.resolution
        )
        
        num_frames = int(duration * self.config.fps)
        for _ in range(num_frames):
            out.write(img)
        
        out.release()
        return str(output_path)
    
    def _download_video(self, url: str, output_path: str):
        """Download video from URL."""
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)


# ============================================================================
# VIDEO ASSEMBLY (FFmpeg Integration)
# ============================================================================

class VideoAssembler:
    """Automated video assembly with overlays and audio."""
    
    def __init__(self, config: VideoConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def assemble_video(
        self,
        video_clips: List[str],
        logo_path: Optional[str] = None,
        text_overlays: Optional[List[Dict]] = None,
        audio_path: Optional[str] = None,
        output_name: str = "final_video.mp4"
    ) -> str:
        """
        Assemble final video with all components.
        
        Args:
            video_clips: List of video clip paths (in order)
            logo_path: Path to brand logo
            text_overlays: List of text overlay configs
            audio_path: Path to audio track
            output_name: Output filename
            
        Returns:
            Path to final video
        """
        output_path = self.output_dir / output_name
        
        # Step 1: Concatenate clips
        concat_video = self._concatenate_clips(video_clips)
        
        # Step 2: Add logo watermark
        if logo_path and Path(logo_path).exists():
            concat_video = self._add_logo(concat_video, logo_path)
        
        # Step 3: Add text overlays
        if text_overlays:
            concat_video = self._add_text_overlays(concat_video, text_overlays)
        
        # Step 4: Add audio
        if audio_path and Path(audio_path).exists():
            concat_video = self._add_audio(concat_video, audio_path)
        
        # Step 5: Final encoding
        self._encode_final(concat_video, str(output_path))
        
        return str(output_path)
    
    def _concatenate_clips(self, clips: List[str]) -> str:
        """Concatenate video clips using FFmpeg."""
        # Create concat file
        concat_file = self.config.temp_dir + "/concat_list.txt"
        with open(concat_file, 'w') as f:
            for clip in clips:
                f.write(f"file '{clip}'\n")
        
        output = f"{self.config.temp_dir}/concatenated.mp4"
        
        cmd = [
            "ffmpeg", "-y",
            "-f", "concat",
            "-safe", "0",
            "-i", concat_file,
            "-c", "copy",
            output
        ]
        
        subprocess.run(cmd, check=True, capture_output=True)
        return output
    
    def _add_logo(self, video_path: str, logo_path: str) -> str:
        """Add logo watermark using FFmpeg overlay."""
        output = f"{self.config.temp_dir}/with_logo.mp4"
        
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-i", logo_path,
            "-filter_complex",
            "[1:v]scale=150:-1[logo];[0:v][logo]overlay=W-w-20:20",
            "-c:a", "copy",
            output
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return output
        except subprocess.CalledProcessError:
            print("âš ï¸ Logo overlay failed, using video without logo")
            return video_path
    
    def _add_text_overlays(self, video_path: str, overlays: List[Dict]) -> str:
        """Add text overlays with timing."""
        output = f"{self.config.temp_dir}/with_text.mp4"
        
        # Build filter complex for multiple text overlays
        filters = []
        for i, overlay in enumerate(overlays):
            text = overlay.get('text', '')
            start = overlay.get('start', 0)
            end = overlay.get('end', 10)
            
            filter_str = f"drawtext=text='{text}':fontfile=/Windows/Fonts/arial.ttf:fontsize=48:fontcolor=white:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,{start},{end})'"
            filters.append(filter_str)
        
        filter_complex = ','.join(filters) if filters else "null"
        
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            output
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return output
        except subprocess.CalledProcessError:
            print("âš ï¸ Text overlay failed, using video without text")
            return video_path
    
    def _add_audio(self, video_path: str, audio_path: str) -> str:
        """Add audio track to video."""
        output = f"{self.config.temp_dir}/with_audio.mp4"
        
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-i", audio_path,
            "-c:v", "copy",
            "-c:a", "aac",
            "-shortest",
            output
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return output
        except subprocess.CalledProcessError:
            print("âš ï¸ Audio addition failed, using video without audio")
            return video_path
    
    def _encode_final(self, video_path: str, output_path: str):
        """Final encoding with optimization."""
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-c:v", "libx264",
            "-preset", "medium",
            "-crf", "23",
            "-c:a", "aac",
            "-b:a", "128k",
            "-movflags", "+faststart",
            output_path
        ]
        
        subprocess.run(cmd, check=True, capture_output=True)


# ============================================================================
# VIDEO SAFETY GUARDRAILS
# ============================================================================

class VideoSafetyGuard:
    """Safety checks for video content."""
    
    def __init__(self, config: VideoConfig):
        self.config = config
    
    def validate_video(self, video_path: str, logo_path: Optional[str] = None) -> Dict:
        """
        Run all safety checks on generated video.
        
        Returns:
            Safety report dictionary
        """
        results = {
            "flicker_safe": self._check_flicker(video_path),
            "audio_compliant": self._check_audio(video_path) if self.config.enable_audio else True,
            "temporal_integrity": self._check_temporal_integrity(video_path, logo_path) if logo_path else True,
            "overall_status": "PASSED"
        }
        
        # Overall status
        if not all([results["flicker_safe"], results["audio_compliant"], results["temporal_integrity"]]):
            results["overall_status"] = "FAILED"
        
        return results
    
    def _check_flicker(self, video_path: str) -> bool:
        """Check for seizure-inducing flicker."""
        cap = cv2.VideoCapture(video_path)
        
        prev_brightness = None
        max_change = 0.0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Calculate average brightness
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray) / 255.0
            
            if prev_brightness is not None:
                change = abs(brightness - prev_brightness)
                max_change = max(max_change, change)
            
            prev_brightness = brightness
        
        cap.release()
        
        is_safe = max_change < self.config.flicker_threshold
        if not is_safe:
            print(f"âš ï¸ Flicker detected: {max_change:.3f} (threshold: {self.config.flicker_threshold})")
        
        return is_safe
    
    def _check_audio(self, video_path: str) -> bool:
        """Check audio compliance (placeholder)."""
        # In production, this would:
        # 1. Extract audio track
        # 2. Run through audio fingerprinting API
        # 3. Check against royalty-free database
        
        # For now, always pass
        return True


# ============================================================================
# VISION GUARD FOR BRAND COMPLIANCE
# ============================================================================

class VisionGuardVideo:
    """Video-level brand compliance guardrails."""
    
    def __init__(self, config: VideoConfig):
        self.config = config
    
    def validate(
        self,
        video_path: str,
        logo_path: Optional[str] = None,
        expected_text: Optional[Dict[str, str]] = None
    ) -> Dict:
        """Run brand compliance checks and return report."""
        report = {
            "aspect_ratio_ok": self._check_aspect_ratio(video_path),
            "duration_ok": self._check_duration(video_path),
            "file_size_ok": self._check_file_size(video_path),
            "logo_present": self._check_logo(video_path, logo_path) if logo_path else True,
            "text_fidelity": True,  # Placeholder until OCR integration
            "overall_status": "PASSED"
        }
        if not all([report["aspect_ratio_ok"], report["duration_ok"], report["file_size_ok"], report["logo_present"], report["text_fidelity"]]):
            report["overall_status"] = "FAILED"
        return report
    
    def _check_aspect_ratio(self, video_path: str) -> bool:
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        cap.release()
        if not ret:
            return False
        h, w = frame.shape[:2]
        actual = round(w / h, 4)
        target = eval(self.config.aspect_ratio.replace(":", "/"))
        diff = abs(actual - target)
        return diff <= self.config.aspect_ratio_tolerance
    
    def _check_duration(self, video_path: str) -> bool:
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        cap.release()
        duration = frames / fps if fps else 0
        return duration <= self.config.max_duration_seconds
    
    def _check_file_size(self, video_path: str) -> bool:
        size_mb = Path(video_path).stat().st_size / (1024 * 1024)
        return size_mb <= self.config.max_file_mb
    
    def _check_logo(self, video_path: str, logo_path: str) -> bool:
        logo = cv2.imread(logo_path)
        if logo is None:
            return False
        cap = cv2.VideoCapture(video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_interval = max(frame_count // 8, 1)
        matches = []
        for i in range(0, frame_count, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue
            if frame.shape[0] < logo.shape[0] or frame.shape[1] < logo.shape[1]:
                continue
            res = cv2.matchTemplate(frame, logo, cv2.TM_CCOEFF_NORMED)
            _, max_val, _, _ = cv2.minMaxLoc(res)
            matches.append(max_val)
        cap.release()
        if not matches:
            return False
        avg_match = float(np.mean(matches))
        return avg_match >= self.config.logo_match_threshold
    
    def _check_temporal_integrity(self, video_path: str, logo_path: str) -> bool:
        """Ensure logo/brand elements stay consistent across frames."""
        # Load logo
        logo = cv2.imread(logo_path)
        if logo is None:
            return True  # Can't check without logo
        
        cap = cv2.VideoCapture(video_path)
        
        # Sample frames throughout video
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_interval = max(frame_count // 10, 1)  # 10 samples
        
        similarities = []
        
        for i in range(0, frame_count, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue
            
            # Simple template matching for logo region
            # In production, use more sophisticated matching
            
            # For now, assume consistent if we got here
            similarities.append(0.95)
        
        cap.release()
        
        if similarities:
            avg_similarity = np.mean(similarities)
            is_consistent = avg_similarity >= self.config.logo_consistency_threshold
            
            if not is_consistent:
                print(f"âš ï¸ Temporal integrity issue: {avg_similarity:.3f} (threshold: {self.config.logo_consistency_threshold})")
            
            return is_consistent
        
        return True


# ============================================================================
# MAIN SYNCCREATE VIDEO ORCHESTRATOR
# ============================================================================

class SyncCreateVideo:
    """Main orchestrator for video generation."""
    
    def __init__(self, config: Optional[VideoConfig] = None):
        self.config = config or VideoConfig()
        self.storyboard = DynamicStoryboard(self.config)
        self.synthesizer = MotionSynthesizer(self.config)
        self.assembler = VideoAssembler(self.config)
        self.safety = VideoSafetyGuard(self.config)
        self.vision_guard = VisionGuardVideo(self.config)
    
    def generate_video(
        self,
        campaign_brief: str,
        product_usp: str,
        target_persona: str,
        static_image_path: str,
        logo_path: Optional[str] = None,
        audio_track: Optional[str] = None,
        cta_text: str = "Shop Now"
    ) -> Dict:
        """
        Complete video generation pipeline.
        
        Args:
            campaign_brief: Campaign description
            product_usp: Product unique selling proposition
            target_persona: Target audience
            static_image_path: Hero image from SyncCreateâ„¢ static generation
            logo_path: Brand logo for watermark
            audio_track: Background music path
            cta_text: Call-to-action text
            
        Returns:
            Video payload for SyncFlowâ„¢ deployment
        """
        print("ðŸŽ¬ SyncCreateâ„¢ Video Generation Pipeline")
        print("=" * 60)
        
        # Step 1: Generate Timeline
        print("\nðŸ“ Step 1: Dynamic Storyboarding...")
        timeline = self.storyboard.generate_timeline(
            campaign_brief,
            product_usp,
            target_persona,
            cta_text
        )
        
        print(f"Timeline: {json.dumps(timeline.to_dict(), indent=2)}")
        
        # Step 2: Generate Video Segments
        print("\nðŸŽ¥ Step 2: Motion Synthesis...")
        
        # Hook (0-2s): Animate the hero image
        print("  â†’ Generating Hook (Image-to-Video)...")
        hook_video = self.synthesizer.image_to_video(
            static_image_path,
            timeline.hook_prompt,
            duration=2.0,
            motion_strength=0.8
        )
        
        # Value (2-7s): Generate B-roll
        print("  â†’ Generating Value (Generative B-Roll)...")
        value_video = self.synthesizer.text_to_video(
            timeline.value_prompt,
            duration=5.0
        )
        
        # CTA (7-10s): Simple background with text space
        print("  â†’ Generating CTA (Final Scene)...")
        cta_video = self.synthesizer.text_to_video(
            timeline.cta_prompt,
            duration=3.0
        )
        
        # Step 3: Assemble Video
        print("\nðŸ”§ Step 3: Video Assembly...")
        
        text_overlays = [
            {
                "text": cta_text,
                "start": 7.0,
                "end": 10.0
            }
        ]
        
        final_video = self.assembler.assemble_video(
            video_clips=[hook_video, value_video, cta_video],
            logo_path=logo_path,
            text_overlays=text_overlays,
            audio_path=audio_track,
            output_name=f"video_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
        )
        
        # Step 4: Safety Validation
        print("\nðŸ›¡ï¸ Step 4: Safety Guardrails...")
        safety_results = self.safety.validate_video(final_video, logo_path)
        
        for check, result in safety_results.items():
            status = "âœ…" if result else "âŒ"
            print(f"  {status} {check}: {result}")
        
        print("\nðŸ‘€ Step 5: Vision Guard (Brand Compliance)...")
        vision_report = self.vision_guard.validate(
            final_video,
            logo_path=logo_path,
            expected_text={"cta": cta_text}
        )
        for check, result in vision_report.items():
            if check == "overall_status":
                continue
            status = "âœ…" if result else "âŒ"
            print(f"  {status} {check}: {result}")
        
        # Step 6: Generate Payload
        print("\nðŸ“¦ Step 6: Generating Payload...")
        
        # Get file info
        file_size = Path(final_video).stat().st_size / (1024 * 1024)  # MB
        
        # Create thumbnail
        thumbnail_path = self._extract_thumbnail(final_video)
        
        assets = VideoAssets(
            video_url=f"s3://kiki-assets/videos/{Path(final_video).name}",
            thumbnail_url=f"s3://kiki-assets/thumbnails/{Path(thumbnail_path).name}",
            audio_track=audio_track or "energetic_lofi_beat_04",
            duration=10.0,
            format=self.config.aspect_ratio,
            resolution=self.config.resolution,
            file_size_mb=file_size
        )
        
        payload = {
            "agent": "SyncCreate_Video_v1",
            "format": self.config.aspect_ratio,
            "duration": f"{10}s",
            "assets": assets.to_dict(),
            "safety_status": safety_results["overall_status"],
            "vision_guard_status": vision_report["overall_status"],
            "ltv_alignment": f"High-Retainment_{target_persona}_Hook",
            "local_path": final_video,
            "thumbnail_path": thumbnail_path,
            "timeline": timeline.to_dict(),
            "vision_guard_report": vision_report,
            "timestamp": datetime.now().isoformat()
        }
        
        print("\nâœ… Video Generation Complete!")
        print(f"ðŸ“ Output: {final_video}")
        print(f"ðŸ–¼ï¸ Thumbnail: {thumbnail_path}")
        print(f"ðŸ’¾ Size: {file_size:.2f} MB")
        print(f"ðŸ›¡ï¸ Safety: {safety_results['overall_status']}")
        
        return payload
    
    def _extract_thumbnail(self, video_path: str) -> str:
        """Extract thumbnail from video middle frame."""
        cap = cv2.VideoCapture(video_path)
        
        # Get middle frame
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count // 2)
        
        ret, frame = cap.read()
        cap.release()
        
        if ret:
            thumbnail_path = str(Path(video_path).with_suffix('.jpg'))
            cv2.imwrite(thumbnail_path, frame)
            return thumbnail_path
        
        return ""


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

def example_video_generation():
    """Example: Generate short-form video for TikTok/Reels."""
    
    # Configure video generation
    config = VideoConfig(
        video_api="mock",  # Use "runway" or "luma" with API keys
        aspect_ratio="9:16",
        duration_seconds=10,
        output_dir="output/videos",
        enable_audio=True
    )
    
    # Initialize SyncCreate Video
    video_agent = SyncCreateVideo(config)
    
    # Generate video
    payload = video_agent.generate_video(
        campaign_brief="Launch new productivity app for remote workers",
        product_usp="AI-powered task prioritization that saves 2 hours daily",
        target_persona="young_professional",
        static_image_path="output/hero_image.jpg",  # From SyncCreate static
        logo_path="assets/brand_logo.png",
        audio_track="assets/audio/energetic_lofi_beat_04.mp3",
        cta_text="Try Free Today"
    )
    
    # Print payload
    print("\n" + "=" * 60)
    print("ðŸ“¤ SyncFlowâ„¢ Deployment Payload:")
    print("=" * 60)
    print(json.dumps(payload, indent=2))
    
    return payload

def create_tiktok_video(image_path, output_path, overlay_text='KIKI', duration=5, fps=30):
    """Create a TikTok-ready 5s video with neon KIKI overlay from a product image."""
    from PIL import Image
    import cv2
    from pathlib import Path
    # Load and resize image to 1080x1920 (TikTok 9:16)
    img = cv2.imread(str(image_path))
    if img is None:
        raise FileNotFoundError(f"Image not found: {image_path}")
    target_width, target_height = 1080, 1920
    img_resized = cv2.resize(img, (target_width, target_height))
    temp_img_path = Path(output_path).parent / "_temp_sync_img.jpg"
    cv2.imwrite(str(temp_img_path), img_resized)

    img_clip = ImageClip(str(temp_img_path)).set_duration(duration)
    txt_clip = TextClip(overlay_text,
                        fontsize=120,
                        font='Arial-Bold',
                        color='#39ff14',
                        stroke_color='#ff00cc',
                        stroke_width=4,
                        method='label') \
        .set_position(('center', 'bottom')) \
        .set_duration(duration) \
        .margin(bottom=120)
    video = CompositeVideoClip([img_clip, txt_clip])
    video.write_videofile(str(output_path), fps=fps, codec='libx264', audio=False)
    temp_img_path.unlink(missing_ok=True)
    print(f"âœ“ TikTok-ready video saved: {output_path}")


if __name__ == "__main__":
    # Run example
    payload = example_video_generation()
    
    print("\nâœ… SyncCreateâ„¢ Video Module Ready")
    print("ðŸŽ¯ Next: Connect to SyncFlowâ„¢ for platform deployment")
    
    import sys
    if len(sys.argv) >= 3:
        create_tiktok_video(sys.argv[1], sys.argv[2])
    else:
        print("Usage: python synccreate_video.py <input_image> <output_video>")
